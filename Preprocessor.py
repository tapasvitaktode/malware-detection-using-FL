import pandas as pd

from sklearn.ensemble import RandomForestClassifier

reduntant_features = []
original_features = []

# Load datasets for each malware type
df_malware = pd.read_csv('./datasets/malware_samples.csv').drop('Unnamed: 0', axis=1).reset_index(drop=True)
df_benign = pd.read_csv('./datasets/benign_samples.csv').drop('Unnamed: 0', axis=1).reset_index(drop=True)


# Combine the two datasets
dataset = pd.concat([df_malware, df_benign], axis=0)

# Randomize the order of instances to ensure randomization
dataset = dataset.sample(frac=1, random_state=42)

original_features = dataset.columns

# Determine the categorical features of the dataset
categorical_features = ['text_Characteristics', 'data_Characteristics', 'rdata_Characteristics',
                        'rsrc_Characteristics', 'reloc_Characteristics', 'pdata_Characteristics',
                        'Characteristics', 'Magic', 'MajorLinkerVersion', 'MinorLinkerVersion',
                        'MajorOperatingSystemVersion', 'MinorOperatingSystemVersion', 'MajorImageVersion',
                        'MinorImageVersion', 'Subsystem', 'DllCharacteristics', 'type']

# Determine continuous (numerical) features
continuous_features = list(set(dataset.columns) - set(categorical_features))

###########################################################################################################
# OUTLIERS HANDLING
# Using 1.5IQR method to remove outliers
###########################################################################################################

for column in continuous_features:

    # Quartile extraction
    col_df = dataset[column]
    q1 = col_df.quantile(0.25)
    q3 = col_df.quantile(0.75)

    # thresholds calculation
    iqr15 = (q3 - q1) * 1.5
    lower = q1 - iqr15
    upper = q3 + iqr15

    # Clamping
    dataset.loc[dataset[column] > upper, column] = upper
    dataset.loc[dataset[column] < lower, column] = lower

############################################################################################################
# NORMALIZATION HANDLING
# Using Range Normalization with low=0 and high=1
############################################################################################################


def fit_minmax_normalizer(df, dictionary, cont_features=list()):

    for col in cont_features:
        min_v = df[col].min()
        max_v = df[col].max()

        dictionary[col] = (min_v, max_v)


def min_max_normalize(df, cont_features=list()):

    for col in cont_features:

        min_v, max_v = max_min_per_feature[col]

        def min_max(n):
            if max_v != min_v:
                return (n - min_v) / (max_v - min_v)
            else:
                return (n - min_v) / 1e-8

        try:

            df[col] = df[col].apply(min_max)

        except Exception:
            print('Error in row: '+col)


# a dictionary to store max and min per feature for future dataset normalization
max_min_per_feature = dict()

# Fit the normalizer with max and mins
fit_minmax_normalizer(dataset, dictionary=max_min_per_feature, cont_features=continuous_features)

min_max_normalize(dataset, cont_features=continuous_features)

###########################################################################################################
# FEATURE SELECTION
# Decided to choose 10 most important features
##########################################################################################################

classifier = RandomForestClassifier(n_estimators=100, random_state=42)

predictors = dataset.drop('type', axis=1)
target = dataset['type']

classifier.fit(predictors, target)

dicts = []

for i, feature in enumerate(predictors.columns):
    dictionary = {'feature': feature, 'importance_score': classifier.feature_importances_[i]}
    dicts.append(dictionary)

dv = pd.DataFrame.from_dict(dicts)
dv = dv.sort_values(by='importance_score', ascending=False)
dv = dv.head(10).sort_values(by='importance_score', ascending=True)

top_features = list(dv['feature'].unique())
top_features.append('type')


def filter_features(df, cols=list()):
    df = df[cols]
    return df


def handle_nans(df):
    """
    Method used to remove unwanted null values after data collection. \n
    """

    for col in df.columns:

        if col != 'type' and col != 'name':

            df.loc[df[col].isnull(), col] = 0


def preprocess_pefile(df):
    """
    Performs the following functions on the dataset: \n
    1. Handle NaNs (If applicable) \n
    2. Normalize data using training dataset settings \n
    3. Remove unnecessary features to reduce overfitting \n
    """

    handle_nans(df)

    # Normalize dataset
    min_max_normalize(df, cont_features=continuous_features)

    names_df = df['name']
    preprocessed_df = df[top_features[:-1]]

    return names_df, preprocessed_df


def get_benign_samples():

    return dataset[dataset['type'] == 'Benign']


def get_malicious_samples():
    return dataset[dataset['type'] == 'Malicious']


def is_number(n):
    try:

        fnum = float(n)
        return True
    except:
        return False


dataset = filter_features(dataset, top_features)
