import numpy as np
import random

import pandas as pd
import ModelAnalyzer as analyzer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score

from copy import deepcopy
import warnings

import sklearn.model_selection as md

import Preprocessor as pp
from Devices import EdgeDevice


class CentralizedServer:
    name = '[Central Server]'

    dataset = None

    predictions = 0
    true_predictions = 0

    model_preference = 'Logistic Regression'

    tree_classifier = neighbor_classifier = random_forest = naive = logistic = None

    train_x, test_x, train_y, test_y = [None] * 4

    #########################################################################
    # INITIALIZATION METHODS
    ########################################################################

    def __init__(self, model_preference='Decision Tree'):
        """Initializes the selected machine learning model if any selected. \n
           model_preference options: \n
           Decision Tree, KNeighbors, Naive Bayes, Random Forest
        """
        self.model_preference = model_preference

        self.malware_test = None
        self.malware_set = None

        self.benign_test = None
        self.benign_set = None

        self.init_dataset()

    def init_dataset(self, benign_amount=3000, malware_amount=3000):
        """Selects amount of rows from both benign and malware datasets and combines into
        the dataset used for training and splitting
        """

        # Clamping
        if benign_amount >= 3000:
            benign_amount = 3000

        # Clamping
        if malware_amount >= 3000:
            malware_amount = 3000

        if benign_amount <= 10:
            benign_amount = 10

        if malware_amount <= 10:
            malware_amount = 10

        print(self.name, 'Initializing datasets...')

        # Create sets for preselected test data
        self.malware_test = pp.get_malicious_samples().tail(3800 - malware_amount)
        self.benign_test = pp.get_benign_samples().tail(3800 - benign_amount)

        # Create data used for training
        self.malware_set = pp.get_malicious_samples().head(malware_amount)
        self.benign_set = pp.get_benign_samples().head(benign_amount)

        # Merge all instances into one for training
        self.dataset = pd.concat([self.malware_set, self.benign_set], axis=0)

        # Randomize order of instances
        self.dataset = self.dataset.sample(frac=1, random_state=42)

        print(self.name, 'Dataset built')

    def init_models(self):
        match self.model_preference:

            case 'Logistic Regression':

                penalty = str('l1')
                solver = 'liblinear'


                self.logistic = LogisticRegression(warm_start=True, penalty=penalty, random_state=42, max_iter=100,
                                                   C=0.5,
                                                   solver=solver)

            case 'KNeighbors':

                neighbors = input('Enter amount of neighbors (default 1): ') or '1'

                # Unexpected input handling
                if not pp.is_number(neighbors):
                    neighbors = 2

                neighbors = int(neighbors)
                if neighbors < 1:
                    neighbors = 1

                dist_metric = input('Enter option of distance metric: '
                                    '(1) Manhattan (2) Euclidean distance (default 1): ') or '1'

                # Unexpected input handling
                if not pp.is_number(dist_metric):
                    dist_metric = 1

                dist_metric = int(dist_metric)

                if not(1 >= dist_metric <= 2):
                    dist_metric = 1

                self.neighbor_classifier = KNeighborsClassifier(n_neighbors=neighbors, p=dist_metric)

            case 'Naive Bayes':
                self.naive = GaussianNB()

            case 'Random Forest':

                n_estimators = input('Enter amount of estimators (default 100): ') or '100'

                # Unexpected input handling
                if not pp.is_number(n_estimators):
                    n_estimators = 100

                n_estimators = int(n_estimators)
                if n_estimators <= 0:
                    n_estimators = 1

                self.random_forest = RandomForestClassifier(criterion='gini',
                                                            n_estimators=n_estimators, max_depth=5)

 

    #############################################################################################################
    #   METHODS THAT PROMPT THE USER FOR A DECISION (e.g. MODEL SELECTION)
    #############################################################################################################

    def prompt_model(self):
        """
        Selects the global model type
        """

        options = ['Logistic Regression', 'KNeighbors', 'Naive Bayes', 'Random Forest']
        print('')
        print('--------------------------------------------------------------------------------------')
        print(self.name, 'Select option of machine learning model:')
        print('1. Logistic Regression')
        print('2. KNeighbors')
        print('3. Gaussian Naive Bayes')
        print('4. Random Forest')
        print('--------------------------------------------------------------------------------------')

        op_number = input('Enter option: ') or '1'

        # Handle unexpected input
        if not pp.is_number(op_number):
            op_number = 1
        op_number = int(op_number)

        # Handle out of range options
        if not (1 <= op_number <= 4):
            op_number = 1

        self.model_preference = options[op_number - 1]

    def train(self):
        """
        Method to train the dataset with the selected model
        :return:
        """

        print(self.name + ' Model Training')
        print('===================================')
        print('')
        print('Training vs Test proportion: ')
        proportion = input('Enter training dataset percentage (default = 0.75): ') or '0.75'
        print('')
        if not pp.is_number(proportion):
            proportion = 0.75
        self.proportion = float(proportion)

        # Handle uninitialized dataset
        if self.dataset is None:
            print(self.name, 'Datasets have not initialized yet')
            print(self.name, 'Initializing them for you...')
            self.init_dataset()

        # Train and Test data partition
        predictors = self.dataset.drop('type', axis=1)
        target = self.dataset['type']

        self.train_x, self.test_x, self.train_y, self.test_y = md.train_test_split(predictors,
                                                                                   target,
                                                                                   random_state=42,
                                                                                   train_size=self.proportion,
                                                                                   test_size=(1 - self.proportion))

        # Model Fitting
        model = self.get_model()
        model.fit(self.train_x, self.train_y)

        print(self.name, 'Training Done')

    def get_stats(self):


        model = self.get_model()

        prediction = model.predict(self.test_x)

        print(self.name + ' Model statistics:')
        print('=====================================')
        print('Malware training rows:', len(self.malware_set))
        print('Benign training rows:', len(self.benign_set))
        print('Accuracy score:', accuracy_score(self.test_y, prediction))
        print('Recall score:', recall_score(self.test_y, prediction, pos_label='Malicious'))
        print('Precision score', precision_score(self.test_y, prediction, pos_label='Malicious'))
        print('F1-score', f1_score(self.test_y, prediction, pos_label='Malicious'))
        print('Confusion matrix: ')
        print(confusion_matrix(self.test_y, prediction, labels=['Malicious', 'Benign']))

    def predict(self, dataframe):


        # Separate predictors and truth targets
        predictor = dataframe.drop(['type'], axis=1).reindex(columns=self.train_x.columns)
        truth_value = dataframe['type']

        model = self.get_model()

        prediction = model.predict(predictor)

        return prediction, truth_value, confusion_matrix(truth_value, prediction, labels=['Malicious', 'Benign']), \
            accuracy_score(truth_value, prediction)

    def predict_new(self, df):

        predictors = df.reindex(columns=self.train_x.columns)
        model = self.get_model()

        return model.predict(predictors)

    def get_model(self):


        match self.model_preference:
            case "Logistic Regression":
                return self.logistic
            case "KNeighbors":
                return self.neighbor_classifier
            case "Naive Bayes":
                return self.naive
            case "Random Forest":
                return self.random_forest


##############################################################################################################
#     FEDERATED LEARNING APPROACH
##############################################################################################################


def avg_models(models):
    amount = len(models)

    # init coefs
    avg_coef = np.zeros_like(models[0].coef_)

    # init bias
    avg_intercept = 0

    # Sum values
    for model in models:
        avg_coef += model.coef_
        avg_intercept += model.intercept_

    # Divide by amount
    avg_coef /= amount
    avg_intercept /= amount

    return avg_coef, avg_intercept


def weighted_avg_models(models, weights):
    sum_weight = np.sum(weights)

    # init coefs
    avg_coef = np.zeros_like(models[0].coef_)

    # init bias
    avg_intercept = 0

    for i, model in enumerate(models):
        avg_coef += model.coef_ * weights[i]
        avg_intercept += model.intercept_ * weights[i]

    avg_coef /= sum_weight
    avg_intercept /= sum_weight

    return avg_coef, avg_intercept


class FederatedServer:
    name = '[Federated Server]'

    edge_devices = []
    global_model = None
    main_dataframe = None

    model_preference = None

    malware_set = pp.get_malicious_samples()
    benign_set = pp.get_benign_samples()

########################################################################################################################
# INITIALIZATION METHODS
########################################################################################################################
    def __init__(self):
        print('Initializing Federated Central Server')
        self.model_preference = 'Logistic Regression'

    def initial_model_settings(self):

        self.global_model = LogisticRegression(max_iter=10, warm_start=True)

    def init_edge_devices(self, n=1):

        test_set = pp.dataset.tail(600)
        train = pp.dataset.head(len(pp.dataset) - len(test_set))
        train_malware = train[train['type'] == 'Malicious'].sample(3000)
        train_benign = train[train['type'] == 'Benign'].sample(3000)

        # Create edge devices
        for i in range(n):
            edge = EdgeDevice(self, test_set)
            self.edge_devices.append(edge)

        # Choose distribution of data and proportion of samples
        print('---------------------------------------------------------------------------------------------------')
        print('Distribution of data: ')
        print('(1) Data is distributed evenly among devices with same amount of malware samples and benign samples')
        print('(2) Data is distributed evenly among devices but local proportion of malware and benign might be '
              'different')
        print('(3) Data is distributed unevenly among devices and proportion of malware and benign samples might be'
              'different')
        print('---------------------------------------------------------------------------------------------------')

        opt = input('Choose an option (default 1): ') or '1'
        print('')
        if not pp.is_number(opt):
            opt = 1

        opt = int(opt)

        if not (1 <= opt <= 3):
            opt = 1

        ok = False

        # Repeat until all datasets have at least 1 sample
        while not ok:

            if opt == 3:
                self.part_imbdist_imbprop(n=n, training_malware=train_malware, training_benign=train_benign)
            elif opt == 2:
                self.part_evendist_imbprop(n=n, training_malware=train_malware, training_benign=train_benign)
            else:
                self.part_evendist_evenprop(n=n, training_malware=train_malware, training_benign=train_benign)

            # Check for datasets that were assigned 0 samples
            ok = self.no_zeroes()

####################################################################################################################
# DATA PARTITION METHODS
####################################################################################################################

    def part_evendist_evenprop(self, n, training_malware, training_benign):


        t_mal = training_malware.copy(deep=True)
        t_ben = training_benign.copy(deep=True)

        t_mals = np.array_split(t_mal, n)
        t_bens = np.array_split(t_ben, n)

        index = 0
        for edge in self.edge_devices:
            t_ben_ = t_bens[index]
            t_mal_ = t_mals[index]

            joined = pd.concat([t_mal_, t_ben_], axis=0)
            amount = len(joined) // 2
            edge.init_data(joined, amount, amount)

            index += 1

    def part_evendist_imbprop(self, n, training_malware, training_benign):


        t_mal = training_malware.copy(deep=True)
        t_ben = training_benign.copy(deep=True)

        amount_mal = len(t_mal)

        amount_ben = len(t_ben)

        amount_per_n = 6000 // n

        for i in range(n):

            edge = self.edge_devices[i]
            setted = False
            attempts = 0

            while not setted:

                malw_prop = random.randrange(2, 9) / 10

                malw_amount = int(amount_per_n * malw_prop)

                ben_amount = int(amount_per_n * (1 - malw_prop))

                eval_malw = (malw_amount < amount_mal) or (malw_amount <= amount_mal and i != (n - 2))

                eval_ben = (ben_amount < amount_ben) or (ben_amount <= amount_ben and i != (n - 2))

                # If X failed attempts to fit data, just use current remaining dataset proportion
                if attempts > 5:
                    malw_amount = amount_mal
                    ben_amount = amount_ben
                    eval_malw = True
                    eval_ben = True

                if eval_malw and eval_ben:
                    setted = True

                    loc_malw = t_mal.sample(malw_amount)
                    loc_ben = t_ben.sample(ben_amount)

                    # Remove sampled instances from datasets
                    # to avoid repeated instances in different devices
                    t_mal = t_mal.drop(loc_malw.index)
                    t_ben = t_ben.drop(loc_ben.index)

                    t_joined = pd.concat([loc_malw, loc_ben], axis=0)
                    edge.init_data(dataset=t_joined, malw_amount=malw_amount, ben_amount=ben_amount)

                    amount_mal -= malw_amount
                    amount_ben -= ben_amount

                attempts += 1

    def part_imbdist_imbprop(self, n, training_malware, training_benign):

        t_mal = training_malware.copy(deep=True)
        t_ben = training_benign.copy(deep=True)

        amount_mal = len(t_mal)
        amount_ben = len(t_ben)
        amount_per_n = 6000 // n

        for i in range(n):

            edge = self.edge_devices[i]
            setted = False
            attempts = 0

            while not setted:

                # Calculate random amount out of amount per n
                distr = random.randint(2, 9) / 10
                distr *= amount_per_n

                malw_prop = random.randrange(2, 9) / 10

                malw_amount = int(distr * malw_prop)

                ben_amount = int(distr * (1 - malw_prop))

                # Check to avoid emptying datasets before other edges get their data
                eval_malw = (malw_amount < amount_mal) or (malw_amount <= amount_mal and i != (n - 2))
                eval_ben = (ben_amount < amount_ben) or (ben_amount <= amount_ben and i != (n - 2))

                # If X failed attempts to fit data, just use current remaining dataset proportion
                if attempts > 5:
                    malw_amount = amount_mal
                    ben_amount = amount_ben
                    eval_malw = True
                    eval_ben = True

                # If dataset has not been depleted or all devices got their data
                # continue
                if eval_malw and eval_ben:
                    setted = True

                    loc_malw = t_mal.sample(malw_amount)
                    loc_ben = t_ben.sample(ben_amount)

                    # Remove sampled instances from datasets
                    # to avoid repeated instances in different devices
                    t_mal = t_mal.drop(loc_malw.index)
                    t_ben = t_ben.drop(loc_ben.index)

                    t_joined = pd.concat([loc_malw, loc_ben], axis=0)

                    edge.init_data(dataset=t_joined, malw_amount=malw_amount, ben_amount=ben_amount)

                    # Update size of datasets
                    amount_mal -= malw_amount
                    amount_ben -= ben_amount

                attempts += 1

    def no_zeroes(self):

        result = True

        for edge in self.edge_devices:
            if len(edge.local_data) == 0:
                return False

        return result




########################################################################################################################
# FEDERATION PROCESSES
########################################################################################################################

    def federate(self):

        n = input('Enter the amount of iterations (default 1): ') or 1

        # Unexpected input handling
        if not pp.is_number(n):
            n = 1
        n = int(n)

        l_rate = input('Enter learning rate (recommended and default 0.02): ') or 0.02

        # Unexpected input handling
        if not pp.is_number(l_rate):
            l_rate = 0.02
        l_rate = float(l_rate)

        ##############################################################
        # Step 1: Send the global model (without training) to the edge devicese
        ##############################################################
        self.deploy()
        self.command_train()

        models = [edge.local_model for edge in self.edge_devices]
        weights = [len(edge.local_data) for edge in self.edge_devices]

        for iteration in range(n):

            ##############################################################
            # Step 2: Model Training
            # Train local models with local data, this is done for all selected edge devices
            ##############################################################

            participating_devices = np.random.choice(self.edge_devices, size=len(self.edge_devices) // 2,
                                                     replace=False)

            # Train selected devices
            for device in participating_devices:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    device.train(iteration+1)

            # Get average from all models
            avg_coef, avg_intercept = weighted_avg_models(models, weights)

            # Deliver updated model to all edge devices
            for model in models:
                model.coef_ -= l_rate * (model.coef_ - avg_coef)
                model.intercept_ -= l_rate * (model.intercept_ - avg_intercept)

            print('Iteration', iteration+1, 'done')

    def deploy(self, initial=True):
        """
        Deploys the global model back to the edge devices
        """

        for edge_device in self.edge_devices:
            edge_device.local_model = deepcopy(self.global_model)

        if initial:
            for edge_device in self.edge_devices:
                edge_device.local_model = deepcopy(self.global_model)

    def command_train(self, initial=True):
        """
        Tells Edge devices to train their local models with their local
        data
        """

        for edge_device in self.edge_devices:
            edge_device.initial_train()

        print(self.name, 'Finished training local models')

    def federated_server_menu(self):

        finish = False

        while not finish:

            print('Federated Server Menu')
            print('=====================')
            print('')
            print('--------------------------------------------------------------------------------------')
            print('(1) Print chart of federated process')
            print('(2) Switch to edge device')
            print('(3) Finish Simulation')
            print('--------------------------------------------------------------------------------------')
            option = int(input('Enter option: ') or 1)
            print('')
            if option == 1:
                analyzer.federated_learning_visualizer(self.edge_devices)
            elif option == 2:
                print('')
                max_size = len(self.edge_devices)
                print('Select the number of device to switch to')
                for i in range(max_size):
                    print('{}. Device {}'.format(i + 1, self.edge_devices[i].id))

                option = input('Enter option: ')

                # Unexpected input handling
                if not pp.is_number(option):
                    option = 1

                option = int(option)

                if not (1 <= option <= len(self.edge_devices)):
                    option = 1

                self.edge_devices[option - 1].device_menu()
            else:
                finish = True
